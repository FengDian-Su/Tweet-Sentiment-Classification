{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 推特留言情緒分析：比較 Word2Vec 和 Bert 表現\n",
        "\n",
        "資料集名稱：Sentiment140 dataset with 1.6 million tweets\n",
        "\n",
        "資料集來源：https://www.kaggle.com/datasets/kazanova/sentiment140/data"
      ],
      "metadata": {
        "id": "RLbKp1fwBMer"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oKubcRpyAz28"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string, nltk, re\n",
        "from gensim.models import Word2Vec\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "drive.mount('/content/drive', force_remount = True)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "stop_words = set(stopwords.words('english'))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 檔案要先儲存在 google drive 再修改路徑\n",
        "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = 'latin-1', header = None)\n",
        "df = df[[5, 0]]\n",
        "df.columns=['tweet', 'Sentiment']\n",
        "df.loc[df[\"Sentiment\"] == 4, \"Sentiment\"] = 1"
      ],
      "metadata": {
        "id": "b-SYa2-OB4gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "  text = text.lower()\n",
        "  text = ''.join([word for word in text if word not in string.punctuation])\n",
        "  tokens = word_tokenize(text)\n",
        "  tokens = [word for word in tokens if word not in stop_words]\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemma_words = [lemmatizer.lemmatize(w, pos = 'a') for w in tokens]\n",
        "  return ' '.join(lemma_words)\n",
        "\n",
        "df[\"tweet\"] = df[\"tweet\"].apply(preprocess)"
      ],
      "metadata": {
        "id": "MbuDJWShFZ3w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Word2Vec ( Using Gensim Package )"
      ],
      "metadata": {
        "id": "o_5iY1dDFmpx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df[\"tweet\"], df[\"Sentiment\"], test_size = 0.3)\n",
        "sentences = [sentence.split() for sentence in X_train]\n",
        "w2v_model = Word2Vec(sentences, vector_size = 100, window = 5, min_count = 5, workers = 4)"
      ],
      "metadata": {
        "id": "AQRehwr5FsLl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def vectorize(sentence):\n",
        "  words = sentence.split()\n",
        "  words_vecs = [w2v_model.wv[word] for word in words if word in w2v_model.wv]\n",
        "  if len(words_vecs) == 0:\n",
        "    return np.zeros(100)\n",
        "  words_vecs = np.array(words_vecs)\n",
        "  return words_vecs.mean(axis = 0)\n",
        "\n",
        "X_train = np.array([vectorize(sentence) for sentence in X_train])\n",
        "X_test = np.array([vectorize(sentence) for sentence in X_test])"
      ],
      "metadata": {
        "id": "7Xn4_A0-F6yM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LogisticRegression()\n",
        "history = clf.fit(X_train, y_train)\n",
        "prediction = clf.predict(X_test)\n",
        "print(classification_report(y_test, prediction))"
      ],
      "metadata": {
        "id": "nEqG28oIKPi_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Bert ( Using Transformers )"
      ],
      "metadata": {
        "id": "pbqrYha3GCly"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 1)\n",
        "encoded_data = tokenizer(df['tweet'].tolist(), padding = True, truncation = True, return_tensors = 'tf')\n",
        "input_ids = np.array(encoded_data['input_ids'])\n",
        "labels = np.array(df['Sentiment'])"
      ],
      "metadata": {
        "id": "8PLmTirpGJz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_valid, y_train, y_valid = train_test_split(input_ids, labels, test_size = 0.3)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size = 0.5)\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train)).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(32)\n",
        "valid_dataset = tf.data.Dataset.from_tensor_slices((X_valid, y_valid)).batch(32)"
      ],
      "metadata": {
        "id": "ZRORBZZVGRdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate = 1e-5)\n",
        "loss = tf.keras.losses.BinaryCrossentropy(from_logits = True)\n",
        "model.compile(optimizer = optimizer, loss = loss, metrics = ['accuracy'])\n",
        "model.fit(train_dataset, epochs = 1, validation_data = valid_dataset)"
      ],
      "metadata": {
        "id": "6JMhx2HWMA-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "OLu-Y0koGjfn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}